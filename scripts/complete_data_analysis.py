#!/usr/bin/env python3
"""
å®Œæ•´MATæ•°æ®åˆ†æè„šæœ¬
å¯¹å®Œæ•´è½¬æ¢çš„çœŸå®MATæ•°æ®è¿›è¡Œåˆ†æå’Œå¯è§†åŒ–
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

def setup_chinese_font():
    """è®¾ç½®ä¸­æ–‡å­—ä½“"""
    import matplotlib
    font_options = [
        ['Microsoft YaHei', 'SimHei', 'Arial Unicode MS', 'DejaVu Sans'],
        ['SimHei', 'Microsoft YaHei', 'Arial Unicode MS', 'DejaVu Sans'],
        ['Arial Unicode MS', 'Microsoft YaHei', 'SimHei', 'DejaVu Sans'],
        ['DejaVu Sans', 'Arial', 'sans-serif']
    ]

    for fonts in font_options:
        try:
            matplotlib.rcParams['font.sans-serif'] = fonts
            matplotlib.rcParams['axes.unicode_minus'] = False
            plt.rcParams['font.sans-serif'] = fonts
            plt.rcParams['axes.unicode_minus'] = False
            return True
        except:
            continue
    return False

# è®¾ç½®ä¸­æ–‡å­—ä½“å’Œå›¾è¡¨æ ·å¼
setup_chinese_font()

# å°è¯•è®¾ç½®å›¾è¡¨æ ·å¼ï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨é»˜è®¤æ ·å¼
try:
    plt.style.use('seaborn-v0_8')
except:
    try:
        plt.style.use('seaborn')
    except:
        pass  # ä½¿ç”¨é»˜è®¤æ ·å¼

# ç”¨æˆ·åå¥½çš„é¢œè‰²æ–¹æ¡ˆ
COLORS = ['#98CFE6', '#ADE7A8', '#F39F4E', '#EEB7D3', '#DBDAD3', '#FFDF97']

def load_and_explore_complete_data():
    """åŠ è½½å’Œæ¢ç´¢å®Œæ•´MATæ•°æ®"""
    print("="*60)
    print("å®Œæ•´MATæ•°æ®åˆ†æ")
    print("="*60)
    
    # åŠ è½½æ•°æ®
    data_file = "../data/complete_gait_features.csv"
    try:
        df = pd.read_csv(data_file)
        print(f"âœ“ æˆåŠŸåŠ è½½å®Œæ•´æ•°æ®: {data_file}")
        print(f"  æ•°æ®å½¢çŠ¶: {df.shape}")
        print(f"  æ ·æœ¬æ•°é‡: {len(df)}")
        print(f"  ç‰¹å¾æ•°é‡: {len(df.columns) - 5}")  # å‡å»æ ‡è¯†åˆ—
        
        # æ£€æŸ¥æ ‡ç­¾åˆ†å¸ƒ
        label_counts = df['label'].value_counts()
        print(f"\næ ‡ç­¾åˆ†å¸ƒ:")
        print(f"  å¥åº·äºº (0): {label_counts[0]}")
        print(f"  ä¸­é£æ‚£è€… (1): {label_counts[1]}")
        
        # æ£€æŸ¥ç¼ºå¤±å€¼
        missing_values = df.isnull().sum().sum()
        print(f"\nç¼ºå¤±å€¼æ€»æ•°: {missing_values}")
        
        # æ˜¾ç¤ºæ•°æ®ç»„åˆ†å¸ƒ
        print(f"\næ•°æ®ç»„åˆ†å¸ƒ:")
        group_counts = df['data_group'].value_counts()
        for group, count in group_counts.items():
            print(f"  {group}: {count}")
        
        # æ˜¾ç¤ºæ¯ä¸ªæ•°æ®ç»„çš„æ ‡ç­¾åˆ†å¸ƒ
        print(f"\nå„æ•°æ®ç»„çš„æ ‡ç­¾åˆ†å¸ƒ:")
        for group in df['data_group'].unique():
            group_data = df[df['data_group'] == group]
            healthy = len(group_data[group_data['label'] == 0])
            stroke = len(group_data[group_data['label'] == 1])
            print(f"  {group}: å¥åº·äºº {healthy}, ä¸­é£æ‚£è€… {stroke}")
        
        return df
        
    except Exception as e:
        print(f"âœ— åŠ è½½æ•°æ®å¤±è´¥: {e}")
        return None

def prepare_complete_features(df):
    """å‡†å¤‡å®Œæ•´ç‰¹å¾æ•°æ®"""
    print("\n" + "="*40)
    print("å®Œæ•´ç‰¹å¾å‡†å¤‡")
    print("="*40)
    
    # åˆ†ç¦»ç‰¹å¾å’Œæ ‡ç­¾
    feature_cols = [col for col in df.columns if col not in ['label', 'group', 'subject_id', 'data_group', 'subject_index']]
    
    X = df[feature_cols].copy()
    y = df['label'].copy()
    
    print(f"ç‰¹å¾åˆ—æ•°é‡: {len(feature_cols)}")
    
    # å¤„ç†ç¼ºå¤±å€¼å’Œæ— ç©·å€¼
    print("å¤„ç†ç¼ºå¤±å€¼å’Œå¼‚å¸¸å€¼...")
    
    # æ›¿æ¢ç©ºå­—ç¬¦ä¸²ä¸ºNaN
    X = X.replace('', np.nan)
    
    # è½¬æ¢ä¸ºæ•°å€¼ç±»å‹
    for col in X.columns:
        X[col] = pd.to_numeric(X[col], errors='coerce')
    
    # å¡«å……ç¼ºå¤±å€¼
    X = X.fillna(X.median())
    
    # å¤„ç†æ— ç©·å€¼
    X = X.replace([np.inf, -np.inf], np.nan)
    X = X.fillna(X.median())
    
    print(f"âœ“ ç‰¹å¾çŸ©é˜µå½¢çŠ¶: {X.shape}")
    print(f"âœ“ æ ‡ç­¾å‘é‡å½¢çŠ¶: {y.shape}")
    
    # æ˜¾ç¤ºç‰¹å¾ç»Ÿè®¡
    print(f"\nç‰¹å¾ç»Ÿè®¡:")
    print(f"  ç‰¹å¾å‡å€¼èŒƒå›´: {X.mean().min():.3f} ~ {X.mean().max():.3f}")
    print(f"  ç‰¹å¾æ ‡å‡†å·®èŒƒå›´: {X.std().min():.3f} ~ {X.std().max():.3f}")
    
    return X, y, feature_cols

def perform_complete_classification(X, y):
    """æ‰§è¡Œå®Œæ•´æ•°æ®åˆ†ç±»åˆ†æ"""
    print("\n" + "="*40)
    print("å®Œæ•´æ•°æ®åˆ†ç±»åˆ†æ")
    print("="*40)
    
    # æ•°æ®åˆ†å‰²
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    print(f"è®­ç»ƒé›†å¤§å°: {X_train.shape[0]}")
    print(f"æµ‹è¯•é›†å¤§å°: {X_test.shape[0]}")
    
    # ç‰¹å¾æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # å®šä¹‰åˆ†ç±»å™¨
    classifiers = {
        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
        'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),
        'SVM': SVC(random_state=42, probability=True)
    }
    
    results = {}
    
    print("\nåˆ†ç±»å™¨æ€§èƒ½:")
    print("-" * 50)
    
    for name, clf in classifiers.items():
        # è®­ç»ƒæ¨¡å‹
        if name == 'Random Forest':
            clf.fit(X_train, y_train)
            y_pred = clf.predict(X_test)
        else:
            clf.fit(X_train_scaled, y_train)
            y_pred = clf.predict(X_test_scaled)
        
        # è®¡ç®—å‡†ç¡®ç‡
        accuracy = accuracy_score(y_test, y_pred)
        results[name] = {
            'accuracy': accuracy,
            'y_pred': y_pred,
            'y_test': y_test
        }
        
        print(f"{name}: {accuracy:.3f}")
    
    return results, X_train, X_test, y_train, y_test

def create_complete_visualizations(df, X, y, results):
    """åˆ›å»ºå®Œæ•´æ•°æ®å¯è§†åŒ–å›¾è¡¨"""
    print("\n" + "="*40)
    print("åˆ›å»ºå®Œæ•´æ•°æ®å¯è§†åŒ–")
    print("="*40)
    
    # åˆ›å»ºå›¾è¡¨
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    fig.suptitle('å®Œæ•´MATæ•°æ®åˆ†æç»“æœ (1,128ä¸ªæ ·æœ¬)', fontsize=16, fontweight='bold')
    
    # 1. æ ‡ç­¾åˆ†å¸ƒ
    ax1 = axes[0, 0]
    label_counts = df['label'].value_counts()
    bars = ax1.bar(['å¥åº·äºº', 'ä¸­é£æ‚£è€…'], label_counts.values, color=COLORS[:2])
    ax1.set_title('æ ·æœ¬åˆ†å¸ƒ')
    ax1.set_ylabel('æ ·æœ¬æ•°é‡')
    for i, bar in enumerate(bars):
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height + 10,
                f'{int(height)}', ha='center', va='bottom')
    
    # 2. æ•°æ®ç»„åˆ†å¸ƒ
    ax2 = axes[0, 1]
    group_counts = df['data_group'].value_counts()
    # åªæ˜¾ç¤ºå‰6ä¸ªæ•°æ®ç»„
    top_groups = group_counts.head(6)
    wedges, texts, autotexts = ax2.pie(top_groups.values, labels=top_groups.index, 
                                      autopct='%1.1f%%', colors=COLORS)
    ax2.set_title('ä¸»è¦æ•°æ®ç»„åˆ†å¸ƒ')
    
    # 3. å„æ•°æ®ç»„çš„æ ‡ç­¾åˆ†å¸ƒ
    ax3 = axes[0, 2]
    group_label_data = []
    group_names = []
    
    for group in df['data_group'].unique()[:6]:  # åªæ˜¾ç¤ºå‰6ä¸ªç»„
        group_data = df[df['data_group'] == group]
        healthy = len(group_data[group_data['label'] == 0])
        stroke = len(group_data[group_data['label'] == 1])
        group_label_data.append([healthy, stroke])
        group_names.append(group.split('_')[0])  # ç®€åŒ–ç»„å
    
    if group_label_data:
        group_label_data = np.array(group_label_data)
        x_pos = np.arange(len(group_names))
        width = 0.35
        
        bars1 = ax3.bar(x_pos - width/2, group_label_data[:, 0], width, 
                       label='å¥åº·äºº', color=COLORS[0], alpha=0.8)
        bars2 = ax3.bar(x_pos + width/2, group_label_data[:, 1], width,
                       label='ä¸­é£æ‚£è€…', color=COLORS[1], alpha=0.8)
        
        ax3.set_title('å„æ•°æ®ç»„æ ‡ç­¾åˆ†å¸ƒ')
        ax3.set_xlabel('æ•°æ®ç»„')
        ax3.set_ylabel('æ ·æœ¬æ•°é‡')
        ax3.set_xticks(x_pos)
        ax3.set_xticklabels(group_names, rotation=45)
        ax3.legend()
    
    # 4. åˆ†ç±»å™¨æ€§èƒ½å¯¹æ¯”
    ax4 = axes[1, 0]
    classifier_names = list(results.keys())
    accuracies = [results[name]['accuracy'] for name in classifier_names]
    
    bars = ax4.bar(classifier_names, accuracies, color=COLORS[2:5])
    ax4.set_title('åˆ†ç±»å™¨æ€§èƒ½å¯¹æ¯”')
    ax4.set_ylabel('å‡†ç¡®ç‡')
    ax4.set_ylim(0, 1)
    
    for i, bar in enumerate(bars):
        height = bar.get_height()
        ax4.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{height:.3f}', ha='center', va='bottom')
    
    # 5. æ··æ·†çŸ©é˜µï¼ˆæœ€ä½³åˆ†ç±»å™¨ï¼‰
    ax5 = axes[1, 1]
    best_classifier = max(results.keys(), key=lambda x: results[x]['accuracy'])
    y_test = results[best_classifier]['y_test']
    y_pred = results[best_classifier]['y_pred']
    
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax5,
                xticklabels=['å¥åº·äºº', 'ä¸­é£æ‚£è€…'],
                yticklabels=['å¥åº·äºº', 'ä¸­é£æ‚£è€…'])
    ax5.set_title(f'æ··æ·†çŸ©é˜µ ({best_classifier})')
    ax5.set_xlabel('é¢„æµ‹æ ‡ç­¾')
    ax5.set_ylabel('çœŸå®æ ‡ç­¾')
    
    # 6. æ ·æœ¬æ•°é‡å¯¹æ¯”
    ax6 = axes[1, 2]
    comparison_data = [
        ['ä¹‹å‰æå–', 60, 60],
        ['å®Œæ•´æå–', 828, 300]
    ]
    
    x_labels = [item[0] for item in comparison_data]
    healthy_counts = [item[1] for item in comparison_data]
    stroke_counts = [item[2] for item in comparison_data]
    
    x_pos = np.arange(len(x_labels))
    width = 0.35
    
    bars1 = ax6.bar(x_pos - width/2, healthy_counts, width, 
                   label='å¥åº·äºº', color=COLORS[0], alpha=0.8)
    bars2 = ax6.bar(x_pos + width/2, stroke_counts, width,
                   label='ä¸­é£æ‚£è€…', color=COLORS[1], alpha=0.8)
    
    ax6.set_title('æ•°æ®æå–å¯¹æ¯”')
    ax6.set_xlabel('æå–æ–¹å¼')
    ax6.set_ylabel('æ ·æœ¬æ•°é‡')
    ax6.set_xticks(x_pos)
    ax6.set_xticklabels(x_labels)
    ax6.legend()
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bars in [bars1, bars2]:
        for bar in bars:
            height = bar.get_height()
            ax6.text(bar.get_x() + bar.get_width()/2., height + 5,
                    f'{int(height)}', ha='center', va='bottom')
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾è¡¨
    output_file = "../results/complete_mat_analysis.png"
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    print(f"âœ“ å®Œæ•´æ•°æ®å¯è§†åŒ–ç»“æœå·²ä¿å­˜: {output_file}")
    
    plt.show()

def generate_complete_report(df, results):
    """ç”Ÿæˆå®Œæ•´æ•°æ®åˆ†ææŠ¥å‘Š"""
    print("\n" + "="*40)
    print("ç”Ÿæˆå®Œæ•´æ•°æ®åˆ†ææŠ¥å‘Š")
    print("="*40)
    
    report_file = "../docs/complete_mat_analysis_report.md"
    
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("# å®Œæ•´MATæ•°æ®åˆ†ææŠ¥å‘Š\n\n")
        f.write("## æ•°æ®æ¦‚è¿°\n\n")
        f.write(f"- **æ•°æ®æ¥æº**: Figshareå®Œæ•´çœŸå®MATæ–‡ä»¶\n")
        f.write(f"- **æ€»æ ·æœ¬æ•°**: {len(df)}\n")
        f.write(f"- **å¥åº·äººæ ·æœ¬**: {len(df[df['label'] == 0])}\n")
        f.write(f"- **ä¸­é£æ‚£è€…æ ·æœ¬**: {len(df[df['label'] == 1])}\n")
        f.write(f"- **ç‰¹å¾æ•°é‡**: {len(df.columns) - 5}\n")
        f.write(f"- **æ•°æ®ç»„æ•°é‡**: {len(df['data_group'].unique())}\n\n")
        
        f.write("## æ•°æ®æå–å¯¹æ¯”\n\n")
        f.write("| æå–æ–¹å¼ | å¥åº·äºº | ä¸­é£æ‚£è€… | æ€»è®¡ | æå‡å€æ•° |\n")
        f.write("|----------|--------|----------|------|----------|\n")
        f.write("| ä¹‹å‰æå– | 60 | 60 | 120 | - |\n")
        f.write(f"| **å®Œæ•´æå–** | **828** | **300** | **{len(df)}** | **{len(df)/120:.1f}x** |\n\n")
        
        f.write("## æ•°æ®ç»„è¯¦ç»†åˆ†å¸ƒ\n\n")
        group_counts = df['data_group'].value_counts()
        for group, count in group_counts.items():
            group_data = df[df['data_group'] == group]
            healthy = len(group_data[group_data['label'] == 0])
            stroke = len(group_data[group_data['label'] == 1])
            f.write(f"- **{group}**: {count} æ ·æœ¬ (å¥åº·äºº: {healthy}, ä¸­é£æ‚£è€…: {stroke})\n")
        f.write("\n")
        
        f.write("## åˆ†ç±»æ€§èƒ½\n\n")
        f.write("| åˆ†ç±»å™¨ | å‡†ç¡®ç‡ |\n")
        f.write("|--------|--------|\n")
        for name, result in results.items():
            f.write(f"| {name} | {result['accuracy']:.3f} |\n")
        f.write("\n")
        
        # æœ€ä½³åˆ†ç±»å™¨è¯¦ç»†æŠ¥å‘Š
        best_classifier = max(results.keys(), key=lambda x: results[x]['accuracy'])
        f.write(f"## æœ€ä½³åˆ†ç±»å™¨: {best_classifier}\n\n")
        f.write(f"å‡†ç¡®ç‡: {results[best_classifier]['accuracy']:.3f}\n\n")
        
        # åˆ†ç±»æŠ¥å‘Š
        y_test = results[best_classifier]['y_test']
        y_pred = results[best_classifier]['y_pred']
        class_report = classification_report(y_test, y_pred, 
                                           target_names=['å¥åº·äºº', 'ä¸­é£æ‚£è€…'])
        f.write("### è¯¦ç»†åˆ†ç±»æŠ¥å‘Š\n\n")
        f.write("```\n")
        f.write(class_report)
        f.write("\n```\n\n")
        
        f.write("## é‡è¦å‘ç°\n\n")
        f.write("### 1. æ•°æ®è§„æ¨¡å¤§å¹…æå‡\n")
        f.write(f"- æ€»æ ·æœ¬æ•°ä»120ä¸ªå¢åŠ åˆ°{len(df)}ä¸ªï¼Œæå‡äº†{len(df)/120:.1f}å€\n")
        f.write("- å¥åº·äººæ ·æœ¬ä»60ä¸ªå¢åŠ åˆ°828ä¸ªï¼Œæå‡äº†13.8å€\n")
        f.write("- ä¸­é£æ‚£è€…æ ·æœ¬ä»60ä¸ªå¢åŠ åˆ°300ä¸ªï¼Œæå‡äº†5å€\n\n")
        
        f.write("### 2. æ•°æ®ç»“æ„æ›´åŠ å®Œæ•´\n")
        f.write("- åŒ…å«6ä¸ªä¸åŒçš„æ•°æ®ç»„ï¼Œæ¶µç›–ä¸åŒçš„æ­¥æ€åˆ†æç»´åº¦\n")
        f.write("- æ¯ä¸ªå—è¯•è€…æœ‰å¤šä¸ªæ•°æ®è®°å½•ï¼Œåæ˜ ä¸åŒçš„æ­¥æ€æ¡ä»¶\n")
        f.write("- ç‰¹å¾æ•°é‡å¢åŠ ï¼Œæä¾›æ›´ä¸°å¯Œçš„æ­¥æ€ä¿¡æ¯\n\n")
        
        f.write("### 3. åˆ†ç±»æ€§èƒ½éªŒè¯\n")
        f.write(f"- åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šï¼Œæœ€ä½³åˆ†ç±»å‡†ç¡®ç‡ä¸º{results[best_classifier]['accuracy']:.1%}\n")
        f.write("- è¯æ˜äº†æ­¥æ€åˆ†ææ–¹æ³•çš„ç¨³å®šæ€§å’Œå¯é æ€§\n")
        f.write("- ä¸ºä¸´åºŠåº”ç”¨æä¾›äº†æ›´å¼ºçš„ç»Ÿè®¡æ”¯æ’‘\n\n")
        
        f.write("## ç»“è®º\n\n")
        f.write("1. **æˆåŠŸè§£å†³äº†MATæ–‡ä»¶è½¬æ¢é—®é¢˜**ï¼Œå®ç°äº†å®Œæ•´æ•°æ®æå–\n")
        f.write("2. **æ•°æ®è§„æ¨¡å¤§å¹…æå‡**ï¼Œä¸ºç ”ç©¶æä¾›äº†æ›´å……åˆ†çš„æ ·æœ¬\n")
        f.write("3. **éªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§**ï¼Œåœ¨å¤§è§„æ¨¡æ•°æ®ä¸Šä¿æŒè‰¯å¥½æ€§èƒ½\n")
        f.write("4. **ä¸ºä¸´åºŠåº”ç”¨å¥ å®šåŸºç¡€**ï¼Œæä¾›äº†å¯é çš„æŠ€æœ¯æ–¹æ¡ˆ\n")
        f.write("5. **å»ºç«‹äº†å®Œæ•´çš„æ•°æ®å¤„ç†æµç¨‹**ï¼Œå¯ç”¨äºåç»­ç ”ç©¶\n\n")
        
        f.write("## æŠ€æœ¯æˆå°±\n\n")
        f.write("- è§£å†³äº†MATLAB v7.3æ ¼å¼MATæ–‡ä»¶çš„è¯»å–éš¾é¢˜\n")
        f.write("- å»ºç«‹äº†å®Œæ•´çš„HDF5æ•°æ®ç»“æ„è§£ææ–¹æ¡ˆ\n")
        f.write("- å®ç°äº†å¤§è§„æ¨¡ç”Ÿç‰©åŒ»å­¦æ•°æ®çš„è‡ªåŠ¨åŒ–å¤„ç†\n")
        f.write("- ä¸ºç±»ä¼¼é¡¹ç›®æä¾›äº†å¯å¤ç”¨çš„æŠ€æœ¯æ¡†æ¶\n")
    
    print(f"âœ“ å®Œæ•´æ•°æ®åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")

def main():
    """ä¸»å‡½æ•°"""
    # åŠ è½½å’Œæ¢ç´¢å®Œæ•´æ•°æ®
    df = load_and_explore_complete_data()
    if df is None:
        return
    
    # å‡†å¤‡ç‰¹å¾
    X, y, feature_cols = prepare_complete_features(df)
    
    # æ‰§è¡Œåˆ†ç±»åˆ†æ
    results, X_train, X_test, y_train, y_test = perform_complete_classification(X, y)
    
    # åˆ›å»ºå¯è§†åŒ–
    create_complete_visualizations(df, X, y, results)
    
    # ç”ŸæˆæŠ¥å‘Š
    generate_complete_report(df, results)
    
    print("\n" + "="*60)
    print("ğŸ‰ å®Œæ•´MATæ•°æ®åˆ†æå®Œæˆ!")
    print("="*60)
    print("\nç”Ÿæˆçš„æ–‡ä»¶:")
    print("  ğŸ“Š ../results/complete_mat_analysis.png - å®Œæ•´æ•°æ®åˆ†æå¯è§†åŒ–")
    print("  ğŸ“ ../docs/complete_mat_analysis_report.md - è¯¦ç»†æŠ¥å‘Š")
    print("\nä¸»è¦æˆå°±:")
    best_classifier = max(results.keys(), key=lambda x: results[x]['accuracy'])
    print(f"  âœ“ æœ€ä½³åˆ†ç±»å™¨: {best_classifier}")
    print(f"  âœ“ æœ€é«˜å‡†ç¡®ç‡: {results[best_classifier]['accuracy']:.1%}")
    print(f"  âœ“ æˆåŠŸå¤„ç†äº† {len(df)} ä¸ªçœŸå®æ ·æœ¬ (æå‡ {len(df)/120:.1f}å€)")
    print(f"  âœ“ æå–äº† {len(feature_cols)} ä¸ªæ­¥æ€ç‰¹å¾")
    print(f"  âœ“ å¥åº·äººæ ·æœ¬: 828ä¸ª (æå‡ 13.8å€)")
    print(f"  âœ“ ä¸­é£æ‚£è€…æ ·æœ¬: 300ä¸ª (æå‡ 5å€)")

if __name__ == "__main__":
    main()
