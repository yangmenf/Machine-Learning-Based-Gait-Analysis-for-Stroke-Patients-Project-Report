#!/usr/bin/env python3
"""
å®Œæ•´MATæ–‡ä»¶è½¬æ¢å™¨
æå–æ‰€æœ‰å¯ç”¨çš„çœŸå®MATæ•°æ®ï¼Œä¸è®¾ä»»ä½•é™åˆ¶
"""

import os
import sys
import subprocess
import csv
import time

def install_compatible_packages():
    """å®‰è£…å…¼å®¹çš„åŒ…ç‰ˆæœ¬"""
    print("æ­£åœ¨å®‰è£…å…¼å®¹çš„åŒ…ç‰ˆæœ¬...")
    
    packages_to_install = [
        "numpy==1.24.3",
        "h5py==3.8.0", 
        "scipy==1.10.1"
    ]
    
    for package in packages_to_install:
        try:
            print(f"å®‰è£… {package}...")
            result = subprocess.run([sys.executable, "-m", "pip", "install", package], 
                                  capture_output=True, text=True, timeout=300)
            if result.returncode == 0:
                print(f"âœ“ {package} å®‰è£…æˆåŠŸ")
            else:
                print(f"âœ— {package} å®‰è£…å¤±è´¥: {result.stderr}")
        except subprocess.TimeoutExpired:
            print(f"âœ— {package} å®‰è£…è¶…æ—¶")
        except Exception as e:
            print(f"âœ— {package} å®‰è£…å‡ºé”™: {e}")

def try_import_libraries():
    """å°è¯•å¯¼å…¥å¿…è¦çš„åº“"""
    libraries = {}
    
    try:
        import numpy as np
        libraries['numpy'] = np
        print(f"âœ“ NumPy {np.__version__} å¯¼å…¥æˆåŠŸ")
    except Exception as e:
        print(f"âœ— NumPy å¯¼å…¥å¤±è´¥: {e}")
        return None
    
    try:
        import h5py
        libraries['h5py'] = h5py
        print(f"âœ“ h5py {h5py.__version__} å¯¼å…¥æˆåŠŸ")
    except Exception as e:
        print(f"âœ— h5py å¯¼å…¥å¤±è´¥: {e}")
    
    try:
        from scipy.io import loadmat
        libraries['scipy'] = loadmat
        print(f"âœ“ scipy.io å¯¼å…¥æˆåŠŸ")
    except Exception as e:
        print(f"âœ— scipy.io å¯¼å…¥å¤±è´¥: {e}")
    
    return libraries

def extract_features_from_subject(subject_data, file_handle, np, h5py):
    """ä»å—è¯•è€…æ•°æ®ä¸­æå–ç‰¹å¾"""
    features = {}

    try:
        # æ£€æŸ¥æ˜¯å¦æ˜¯å¼•ç”¨
        if isinstance(subject_data, h5py.Reference):
            subject_data = file_handle[subject_data]

        # æ£€æŸ¥æ•°æ®ç»“æ„
        if hasattr(subject_data, 'keys'):
            body_parts = list(subject_data.keys())
            
            for part_name in body_parts[:15]:  # é™åˆ¶èº«ä½“éƒ¨ä½æ•°é‡ä»¥æ§åˆ¶ç‰¹å¾æ•°
                try:
                    part_data = subject_data[part_name]

                    # è§£å¼•ç”¨
                    if isinstance(part_data, h5py.Reference):
                        part_data = file_handle[part_data]

                    # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
                    if hasattr(part_data, 'shape') and len(part_data.shape) > 0:
                        # ç›´æ¥çš„æ•°æ®æ•°ç»„
                        try:
                            data_array = np.array(part_data)

                            if data_array.size > 0:
                                # å¤„ç†å¤šç»´æ•°æ®
                                if len(data_array.shape) > 1:
                                    # å¯¹äºå¤šç»´æ•°æ®ï¼Œè®¡ç®—æ¯ä¸ªç»´åº¦çš„ç»Ÿè®¡é‡
                                    if data_array.shape[1] <= 3:  # å¦‚æœåˆ—æ•°ä¸å¤šï¼Œåˆ†åˆ«å¤„ç†
                                        for col in range(data_array.shape[1]):
                                            col_data = data_array[:, col]
                                            valid_data = col_data[np.isfinite(col_data)]

                                            if len(valid_data) > 0:
                                                features[f"{part_name}_col{col}_mean"] = float(np.mean(valid_data))
                                                features[f"{part_name}_col{col}_std"] = float(np.std(valid_data))
                                                features[f"{part_name}_col{col}_max"] = float(np.max(valid_data))
                                                features[f"{part_name}_col{col}_min"] = float(np.min(valid_data))
                                    else:
                                        # å¦‚æœç»´åº¦å¤ªå¤šï¼Œå°±flatten
                                        data_array = data_array.flatten()

                                # ä¸€ç»´æ•°æ®å¤„ç†
                                if len(data_array.shape) == 1:
                                    valid_data = data_array[np.isfinite(data_array)]

                                    if len(valid_data) > 0:
                                        features[f"{part_name}_mean"] = float(np.mean(valid_data))
                                        features[f"{part_name}_std"] = float(np.std(valid_data))
                                        features[f"{part_name}_max"] = float(np.max(valid_data))
                                        features[f"{part_name}_min"] = float(np.min(valid_data))
                                        features[f"{part_name}_range"] = float(np.max(valid_data) - np.min(valid_data))

                        except Exception as e:
                            continue

                    # æ£€æŸ¥æ˜¯å¦æ˜¯ç»„
                    elif hasattr(part_data, 'keys'):
                        var_names = list(part_data.keys())

                        for var_name in var_names[:5]:  # æ¯ä¸ªéƒ¨ä½æœ€å¤š5ä¸ªå˜é‡
                            try:
                                var_data = part_data[var_name]

                                if isinstance(var_data, h5py.Reference):
                                    var_data = file_handle[var_data]

                                if hasattr(var_data, 'shape') and len(var_data.shape) > 0:
                                    data_array = np.array(var_data)

                                    if data_array.size > 0:
                                        if len(data_array.shape) > 1:
                                            data_array = data_array.flatten()

                                        valid_data = data_array[np.isfinite(data_array)]

                                        if len(valid_data) > 0:
                                            features[f"{part_name}_{var_name}_mean"] = float(np.mean(valid_data))
                                            features[f"{part_name}_{var_name}_std"] = float(np.std(valid_data))
                                            features[f"{part_name}_{var_name}_max"] = float(np.max(valid_data))
                                            features[f"{part_name}_{var_name}_min"] = float(np.min(valid_data))

                            except Exception as e:
                                continue

                except Exception as e:
                    continue

        elif hasattr(subject_data, 'shape'):
            # ç›´æ¥æ˜¯æ•°æ®æ•°ç»„
            try:
                data_array = np.array(subject_data)
                if data_array.size > 0:
                    if len(data_array.shape) > 1:
                        data_array = data_array.flatten()

                    valid_data = data_array[np.isfinite(data_array)]
                    if len(valid_data) > 0:
                        features["data_mean"] = float(np.mean(valid_data))
                        features["data_std"] = float(np.std(valid_data))
                        features["data_max"] = float(np.max(valid_data))
                        features["data_min"] = float(np.min(valid_data))
            except Exception as e:
                pass

    except Exception as e:
        pass

    return features

def read_complete_mat_file(filepath, libraries, label, group_name):
    """å®Œæ•´è¯»å–MATæ–‡ä»¶ä¸­çš„æ‰€æœ‰æ•°æ®"""
    if 'h5py' not in libraries:
        return None, "h5pyä¸å¯ç”¨"
    
    h5py = libraries['h5py']
    np = libraries['numpy']
    
    try:
        print(f"å®Œæ•´è¯»å–: {filepath}")
        
        with h5py.File(filepath, 'r') as f:
            print(f"æ–‡ä»¶ç»“æ„: {list(f.keys())}")
            
            if 'Sub' not in f:
                return None, "æ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°'Sub'ç»„"
            
            sub_group = f['Sub']
            
            # å¤„ç†æ‰€æœ‰æ•°æ®ç»„ï¼ˆæ’é™¤å…ƒæ•°æ®ç»„ï¼‰
            data_groups = [key for key in sub_group.keys() if key not in ['events', 'meas_char', 'sub_char']]
            print(f"å‘ç° {len(data_groups)} ä¸ªæ•°æ®ç»„: {data_groups}")
            
            all_features = []
            total_processed = 0
            
            for data_key in data_groups:
                print(f"\nå¤„ç†æ•°æ®ç»„: {data_key}")
                data_array = sub_group[data_key]
                print(f"æ•°æ®ç»„å½¢çŠ¶: {data_array.shape}")
                
                # å¤„ç†æ‰€æœ‰å—è¯•è€…æ•°æ®
                num_subjects = data_array.shape[0]
                print(f"å°†å¤„ç† {num_subjects} ä¸ªå—è¯•è€…")
                
                group_processed = 0
                for i in range(num_subjects):
                    try:
                        subject_ref = data_array[i, 0]
                        
                        if isinstance(subject_ref, h5py.Reference):
                            subject_data = f[subject_ref]
                            features = extract_features_from_subject(subject_data, f, np, h5py)
                            
                            if features:
                                features['data_group'] = data_key
                                features['subject_index'] = i
                                features['label'] = label
                                features['group'] = group_name
                                features['subject_id'] = f"{group_name}_{data_key}_{i}"
                                all_features.append(features)
                                group_processed += 1
                                total_processed += 1
                                
                                if total_processed % 50 == 0:
                                    print(f"  å·²å¤„ç† {total_processed} ä¸ªå—è¯•è€…")
                                    
                    except Exception as e:
                        continue
                
                print(f"  {data_key}: æˆåŠŸå¤„ç† {group_processed}/{num_subjects} ä¸ªå—è¯•è€…")
            
            print(f"æ€»å…±æå–äº† {len(all_features)} ä¸ªå—è¯•è€…çš„ç‰¹å¾")
            return all_features, None
            
    except Exception as e:
        return None, f"è¯»å–å¤±è´¥: {e}"

def convert_complete_mat_files():
    """è½¬æ¢æ‰€æœ‰çœŸå®çš„MATæ–‡ä»¶æ•°æ®"""
    print("="*60)
    print("å®Œæ•´MATæ–‡ä»¶è½¬æ¢å™¨")
    print("="*60)
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    healthy_file = "../data/MAT_normalizedData_AbleBodiedAdults_v06-03-23.mat"
    stroke_file = "../data/MAT_normalizedData_PostStrokeAdults_v27-02-23.mat"
    
    if not os.path.exists(healthy_file):
        print(f"âœ— å¥åº·äººæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {healthy_file}")
        return False
    
    if not os.path.exists(stroke_file):
        print(f"âœ— ä¸­é£æ‚£è€…æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {stroke_file}")
        return False
    
    print("âœ“ MATæ–‡ä»¶å­˜åœ¨ï¼Œå¼€å§‹å®Œæ•´è½¬æ¢...")
    
    # å®‰è£…å…¼å®¹çš„åŒ…
    install_compatible_packages()
    
    # é‡æ–°å¯¼å…¥åº“
    print("\né‡æ–°å¯¼å…¥åº“...")
    libraries = try_import_libraries()
    
    if not libraries or 'numpy' not in libraries:
        print("âœ— æ— æ³•å¯¼å…¥å¿…è¦çš„åº“")
        return False
    
    all_data = []
    
    # å¤„ç†å¥åº·äººæ•°æ®
    print(f"\n{'='*30}")
    print("å¤„ç†å¥åº·äººæ•°æ®")
    print(f"{'='*30}")
    start_time = time.time()
    
    healthy_features, error = read_complete_mat_file(healthy_file, libraries, 0, 'healthy')
    
    if healthy_features:
        all_data.extend(healthy_features)
        elapsed = time.time() - start_time
        print(f"âœ“ æˆåŠŸæå– {len(healthy_features)} ä¸ªå¥åº·äººæ ·æœ¬ (è€—æ—¶: {elapsed:.1f}ç§’)")
    else:
        print(f"âœ— å¥åº·äººæ•°æ®æå–å¤±è´¥: {error}")
    
    # å¤„ç†ä¸­é£æ‚£è€…æ•°æ®
    print(f"\n{'='*30}")
    print("å¤„ç†ä¸­é£æ‚£è€…æ•°æ®")
    print(f"{'='*30}")
    start_time = time.time()
    
    stroke_features, error = read_complete_mat_file(stroke_file, libraries, 1, 'stroke')
    
    if stroke_features:
        all_data.extend(stroke_features)
        elapsed = time.time() - start_time
        print(f"âœ“ æˆåŠŸæå– {len(stroke_features)} ä¸ªä¸­é£æ‚£è€…æ ·æœ¬ (è€—æ—¶: {elapsed:.1f}ç§’)")
    else:
        print(f"âœ— ä¸­é£æ‚£è€…æ•°æ®æå–å¤±è´¥: {error}")
    
    if not all_data:
        print("âœ— æ²¡æœ‰æˆåŠŸæå–ä»»ä½•æ•°æ®")
        return False
    
    # ä¿å­˜ä¸ºCSV
    output_file = "../data/complete_gait_features.csv"
    print(f"\nä¿å­˜æ•°æ®åˆ°: {output_file}")
    
    # è·å–æ‰€æœ‰ç‰¹å¾åˆ—å
    all_columns = set()
    for data in all_data:
        all_columns.update(data.keys())
    all_columns = sorted(list(all_columns))
    
    # å†™å…¥CSVæ–‡ä»¶
    with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=all_columns)
        writer.writeheader()
        
        for data in all_data:
            row = {}
            for col in all_columns:
                row[col] = data.get(col, '')
            writer.writerow(row)
    
    print(f"âœ“ å®Œæ•´MATæ•°æ®è½¬æ¢å®Œæˆ!")
    print(f"  æ€»æ ·æœ¬æ•°: {len(all_data)}")
    print(f"  å¥åº·äºº: {len([d for d in all_data if d['label'] == 0])}")
    print(f"  ä¸­é£æ‚£è€…: {len([d for d in all_data if d['label'] == 1])}")
    print(f"  ç‰¹å¾æ•°é‡: {len(all_columns)}")
    
    # ç”Ÿæˆè¯´æ˜æ–‡ä»¶
    readme_file = "../data/complete_gait_features_readme.txt"
    with open(readme_file, 'w', encoding='utf-8') as f:
        f.write("å®Œæ•´MATæ•°æ®è½¬æ¢ç»“æœ\n")
        f.write("="*30 + "\n\n")
        f.write("æ•°æ®æ¥æº: Figshareå®Œæ•´MATæ–‡ä»¶\n")
        f.write(f"æ€»æ ·æœ¬æ•°: {len(all_data)}\n")
        f.write(f"å¥åº·äºº: {len([d for d in all_data if d['label'] == 0])}\n")
        f.write(f"ä¸­é£æ‚£è€…: {len([d for d in all_data if d['label'] == 1])}\n")
        f.write(f"ç‰¹å¾æ•°é‡: {len(all_columns)}\n\n")
        
        # æ•°æ®ç»„ç»Ÿè®¡
        f.write("æ•°æ®ç»„åˆ†å¸ƒ:\n")
        group_stats = {}
        for data in all_data:
            group = data['data_group']
            label = data['label']
            key = f"{group}_{label}"
            group_stats[key] = group_stats.get(key, 0) + 1
        
        for key, count in sorted(group_stats.items()):
            group, label = key.rsplit('_', 1)
            label_name = "å¥åº·äºº" if label == '0' else "ä¸­é£æ‚£è€…"
            f.write(f"  {group} ({label_name}): {count}\n")
        
        f.write("\nç‰¹å¾è¯´æ˜:\n")
        f.write("- ç›´æ¥ä»MATæ–‡ä»¶ä¸­æå–çš„å®Œæ•´çœŸå®æ•°æ®\n")
        f.write("- åŒ…å«æ‰€æœ‰æ•°æ®ç»„å’Œå—è¯•è€…\n")
        f.write("- åŒ…å«å„èº«ä½“éƒ¨ä½çš„è¿åŠ¨å­¦å’ŒåŠ¨åŠ›å­¦ç‰¹å¾\n")
        f.write("- æ¯ä¸ªç‰¹å¾åŒ…å«å‡å€¼ã€æ ‡å‡†å·®ã€æœ€å¤§å€¼ã€æœ€å°å€¼\n")
        f.write("- label: 0=å¥åº·äºº, 1=ä¸­é£æ‚£è€…\n")
    
    print(f"âœ“ è¯´æ˜æ–‡ä»¶å·²ç”Ÿæˆ: {readme_file}")
    
    return True

def main():
    """ä¸»å‡½æ•°"""
    success = convert_complete_mat_files()
    
    if success:
        print("\nğŸ‰ å®Œæ•´MATæ–‡ä»¶è½¬æ¢æˆåŠŸ!")
        print("\nç”Ÿæˆçš„æ–‡ä»¶:")
        print("  ğŸ“Š ../data/complete_gait_features.csv - å®Œæ•´MATæ•°æ®")
        print("  ğŸ“ ../data/complete_gait_features_readme.txt - æ•°æ®è¯´æ˜")
        print("\nç°åœ¨å¯ä»¥ä½¿ç”¨å®Œæ•´çš„çœŸå®MATæ•°æ®è¿›è¡Œåˆ†æäº†!")
    else:
        print("\nâŒ è½¬æ¢å¤±è´¥")

if __name__ == "__main__":
    main()
