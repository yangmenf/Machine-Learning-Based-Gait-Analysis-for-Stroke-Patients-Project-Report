#!/usr/bin/env python3
"""
ä¸­æ–‡å­—ä½“ä¿®å¤è„šæœ¬
è§£å†³matplotlibä¸­æ–‡æ˜¾ç¤ºé—®é¢˜ï¼Œé‡æ–°ç”Ÿæˆå¯è§†åŒ–ç»“æœ
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

def setup_chinese_font():
    """è®¾ç½®ä¸­æ–‡å­—ä½“"""
    print("æ­£åœ¨é…ç½®ä¸­æ–‡å­—ä½“...")
    
    # å°è¯•å¤šç§ä¸­æ–‡å­—ä½“é…ç½®
    font_options = [
        ['Microsoft YaHei', 'SimHei', 'Arial Unicode MS', 'DejaVu Sans'],
        ['SimHei', 'Microsoft YaHei', 'Arial Unicode MS', 'DejaVu Sans'],
        ['Arial Unicode MS', 'Microsoft YaHei', 'SimHei', 'DejaVu Sans'],
        ['DejaVu Sans', 'Arial', 'sans-serif']  # å¤‡ç”¨é€‰é¡¹
    ]
    
    for fonts in font_options:
        try:
            matplotlib.rcParams['font.sans-serif'] = fonts
            matplotlib.rcParams['axes.unicode_minus'] = False
            plt.rcParams['font.sans-serif'] = fonts
            plt.rcParams['axes.unicode_minus'] = False
            
            # æµ‹è¯•ä¸­æ–‡æ˜¾ç¤º
            fig, ax = plt.subplots(figsize=(1, 1))
            ax.text(0.5, 0.5, 'æµ‹è¯•ä¸­æ–‡', ha='center', va='center')
            plt.close(fig)
            
            print(f"âœ“ æˆåŠŸé…ç½®å­—ä½“: {fonts[0]}")
            return True
        except Exception as e:
            continue
    
    print("âš ï¸ æ— æ³•é…ç½®ä¸­æ–‡å­—ä½“ï¼Œå°†ä½¿ç”¨è‹±æ–‡æ ‡ç­¾")
    return False

def load_data():
    """åŠ è½½å®Œæ•´æ•°æ®"""
    try:
        df = pd.read_csv("../data/complete_gait_features.csv")
        print(f"âœ“ æˆåŠŸåŠ è½½æ•°æ®: {df.shape}")
        return df
    except Exception as e:
        print(f"âœ— åŠ è½½æ•°æ®å¤±è´¥: {e}")
        return None

def prepare_features(df):
    """å‡†å¤‡ç‰¹å¾æ•°æ®"""
    feature_cols = [col for col in df.columns if col not in ['label', 'group', 'subject_id', 'data_group', 'subject_index']]
    
    X = df[feature_cols].copy()
    y = df['label'].copy()
    
    # å¤„ç†ç¼ºå¤±å€¼å’Œå¼‚å¸¸å€¼
    X = X.replace('', np.nan)
    for col in X.columns:
        X[col] = pd.to_numeric(X[col], errors='coerce')
    X = X.fillna(X.median())
    X = X.replace([np.inf, -np.inf], np.nan)
    X = X.fillna(X.median())
    
    return X, y, feature_cols

def perform_classification(X, y):
    """æ‰§è¡Œåˆ†ç±»åˆ†æ"""
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    classifiers = {
        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
        'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),
        'SVM': SVC(random_state=42, probability=True)
    }
    
    results = {}
    for name, clf in classifiers.items():
        if name == 'Random Forest':
            clf.fit(X_train, y_train)
            y_pred = clf.predict(X_test)
        else:
            clf.fit(X_train_scaled, y_train)
            y_pred = clf.predict(X_test_scaled)
        
        accuracy = accuracy_score(y_test, y_pred)
        results[name] = {
            'accuracy': accuracy,
            'y_pred': y_pred,
            'y_test': y_test
        }
    
    return results

def create_fixed_visualization(df, X, y, results, use_chinese=True):
    """åˆ›å»ºä¿®å¤åçš„å¯è§†åŒ–å›¾è¡¨"""
    print("åˆ›å»ºä¿®å¤åçš„å¯è§†åŒ–...")
    
    # ç”¨æˆ·åå¥½çš„é¢œè‰²æ–¹æ¡ˆ
    COLORS = ['#98CFE6', '#ADE7A8', '#F39F4E', '#EEB7D3', '#DBDAD3', '#FFDF97']
    
    # è®¾ç½®æ ‡ç­¾ï¼ˆä¸­æ–‡æˆ–è‹±æ–‡ï¼‰
    if use_chinese:
        labels = {
            'healthy': 'å¥åº·äºº',
            'stroke': 'ä¸­é£æ‚£è€…',
            'sample_dist': 'æ ·æœ¬åˆ†å¸ƒ',
            'sample_count': 'æ ·æœ¬æ•°é‡',
            'data_group_dist': 'ä¸»è¦æ•°æ®ç»„åˆ†å¸ƒ',
            'group_label_dist': 'å„æ•°æ®ç»„æ ‡ç­¾åˆ†å¸ƒ',
            'data_group': 'æ•°æ®ç»„',
            'classifier_perf': 'åˆ†ç±»å™¨æ€§èƒ½å¯¹æ¯”',
            'accuracy': 'å‡†ç¡®ç‡',
            'confusion_matrix': 'æ··æ·†çŸ©é˜µ',
            'predicted_label': 'é¢„æµ‹æ ‡ç­¾',
            'true_label': 'çœŸå®æ ‡ç­¾',
            'data_extract_comp': 'æ•°æ®æå–å¯¹æ¯”',
            'extract_method': 'æå–æ–¹å¼',
            'initial_extract': 'ä¹‹å‰æå–',
            'complete_extract': 'å®Œæ•´æå–',
            'title': 'å®Œæ•´MATæ•°æ®åˆ†æç»“æœ (1,128ä¸ªæ ·æœ¬)'
        }
    else:
        labels = {
            'healthy': 'Healthy',
            'stroke': 'Stroke',
            'sample_dist': 'Sample Distribution',
            'sample_count': 'Sample Count',
            'data_group_dist': 'Main Data Group Distribution',
            'group_label_dist': 'Label Distribution by Data Group',
            'data_group': 'Data Group',
            'classifier_perf': 'Classifier Performance Comparison',
            'accuracy': 'Accuracy',
            'confusion_matrix': 'Confusion Matrix',
            'predicted_label': 'Predicted Label',
            'true_label': 'True Label',
            'data_extract_comp': 'Data Extraction Comparison',
            'extract_method': 'Extraction Method',
            'initial_extract': 'Initial Extract',
            'complete_extract': 'Complete Extract',
            'title': 'Complete MAT Data Analysis Results (1,128 samples)'
        }
    
    # åˆ›å»ºå›¾è¡¨
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    fig.suptitle(labels['title'], fontsize=16, fontweight='bold')
    
    # 1. æ ‡ç­¾åˆ†å¸ƒ
    ax1 = axes[0, 0]
    label_counts = df['label'].value_counts()
    bars = ax1.bar([labels['healthy'], labels['stroke']], label_counts.values, color=COLORS[:2])
    ax1.set_title(labels['sample_dist'], pad=15)
    ax1.set_ylabel(labels['sample_count'])
    # åŠ¨æ€è°ƒæ•´Yè½´ä¸Šé™ï¼Œä¸ºæ•°å­—æ ‡æ³¨ç•™å‡ºç©ºé—´
    max_height = max(label_counts.values)
    ax1.set_ylim(0, max_height * 1.15)
    for bar in bars:
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height + max_height * 0.02,
                f'{int(height)}', ha='center', va='bottom', fontsize=10)
    
    # 2. æ•°æ®ç»„åˆ†å¸ƒ
    ax2 = axes[0, 1]
    group_counts = df['data_group'].value_counts()
    top_groups = group_counts.head(6)
    ax2.pie(top_groups.values, labels=top_groups.index,
            autopct='%1.1f%%', colors=COLORS, textprops={'fontsize': 8})
    ax2.set_title(labels['data_group_dist'], pad=15)
    
    # 3. å„æ•°æ®ç»„çš„æ ‡ç­¾åˆ†å¸ƒ
    ax3 = axes[0, 2]
    group_label_data = []
    group_names = []
    
    for group in df['data_group'].unique()[:6]:
        group_data = df[df['data_group'] == group]
        healthy = len(group_data[group_data['label'] == 0])
        stroke = len(group_data[group_data['label'] == 1])
        group_label_data.append([healthy, stroke])
        group_names.append(group.split('_')[0])
    
    if group_label_data:
        group_label_data = np.array(group_label_data)
        x_pos = np.arange(len(group_names))
        width = 0.35
        
        bars1 = ax3.bar(x_pos - width/2, group_label_data[:, 0], width, 
                       label=labels['healthy'], color=COLORS[0], alpha=0.8)
        bars2 = ax3.bar(x_pos + width/2, group_label_data[:, 1], width,
                       label=labels['stroke'], color=COLORS[1], alpha=0.8)
        
        ax3.set_title(labels['group_label_dist'], pad=15)
        ax3.set_xlabel(labels['data_group'])
        ax3.set_ylabel(labels['sample_count'])
        ax3.set_xticks(x_pos)
        ax3.set_xticklabels(group_names, rotation=45)
        ax3.legend(loc='upper right')
    
    # 4. åˆ†ç±»å™¨æ€§èƒ½å¯¹æ¯”
    ax4 = axes[1, 0]
    classifier_names = list(results.keys())
    accuracies = [results[name]['accuracy'] for name in classifier_names]

    bars = ax4.bar(classifier_names, accuracies, color=COLORS[2:5])
    ax4.set_title(labels['classifier_perf'], pad=20)  # å¢åŠ æ ‡é¢˜é—´è·
    ax4.set_ylabel(labels['accuracy'])
    ax4.set_ylim(0, 1.1)  # å¢åŠ Yè½´ä¸Šé™ï¼Œä¸ºæ•°å­—æ ‡æ³¨ç•™å‡ºç©ºé—´

    for bar in bars:
        height = bar.get_height()
        # è°ƒæ•´æ•°å­—æ ‡æ³¨ä½ç½®ï¼Œé¿å…ä¸æ ‡é¢˜é‡åˆ
        ax4.text(bar.get_x() + bar.get_width()/2., height + 0.02,
                f'{height:.3f}', ha='center', va='bottom', fontsize=10)
    
    # 5. æ··æ·†çŸ©é˜µ
    ax5 = axes[1, 1]
    best_classifier = max(results.keys(), key=lambda x: results[x]['accuracy'])
    y_test = results[best_classifier]['y_test']
    y_pred = results[best_classifier]['y_pred']

    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax5,
                xticklabels=[labels['healthy'], labels['stroke']],
                yticklabels=[labels['healthy'], labels['stroke']],
                annot_kws={'fontsize': 12})
    ax5.set_title(f"{labels['confusion_matrix']} ({best_classifier})", pad=15)
    ax5.set_xlabel(labels['predicted_label'])
    ax5.set_ylabel(labels['true_label'])
    
    # 6. æ•°æ®æå–å¯¹æ¯”
    ax6 = axes[1, 2]
    comparison_data = [
        [labels['initial_extract'], 60, 60],
        [labels['complete_extract'], 828, 300]
    ]

    x_labels = [item[0] for item in comparison_data]
    healthy_counts = [item[1] for item in comparison_data]
    stroke_counts = [item[2] for item in comparison_data]

    x_pos = np.arange(len(x_labels))
    width = 0.35

    bars1 = ax6.bar(x_pos - width/2, healthy_counts, width,
                   label=labels['healthy'], color=COLORS[0], alpha=0.8)
    bars2 = ax6.bar(x_pos + width/2, stroke_counts, width,
                   label=labels['stroke'], color=COLORS[1], alpha=0.8)

    ax6.set_title(labels['data_extract_comp'], pad=15)
    ax6.set_xlabel(labels['extract_method'])
    ax6.set_ylabel(labels['sample_count'])
    ax6.set_xticks(x_pos)
    ax6.set_xticklabels(x_labels)
    ax6.legend(loc='upper left')

    # åŠ¨æ€è°ƒæ•´Yè½´ä¸Šé™ï¼Œä¸ºæ•°å­—æ ‡æ³¨ç•™å‡ºç©ºé—´
    max_height = max(max(healthy_counts), max(stroke_counts))
    ax6.set_ylim(0, max_height * 1.15)

    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bars in [bars1, bars2]:
        for bar in bars:
            height = bar.get_height()
            ax6.text(bar.get_x() + bar.get_width()/2., height + max_height * 0.02,
                    f'{int(height)}', ha='center', va='bottom', fontsize=10)
    
    # è°ƒæ•´å­å›¾é—´è·ï¼Œé¿å…é‡å 
    plt.tight_layout(pad=3.0, h_pad=3.0, w_pad=2.0)

    # ä¿å­˜å›¾è¡¨
    output_file = "../results/complete_mat_analysis_fixed.png"
    plt.savefig(output_file, dpi=300, bbox_inches='tight', facecolor='white')
    print(f"âœ“ ä¿®å¤åçš„å¯è§†åŒ–ç»“æœå·²ä¿å­˜: {output_file}")

    plt.show()

def main():
    """ä¸»å‡½æ•°"""
    print("="*60)
    print("ä¸­æ–‡å­—ä½“ä¿®å¤å’Œå¯è§†åŒ–é‡æ–°ç”Ÿæˆ")
    print("="*60)
    
    # è®¾ç½®ä¸­æ–‡å­—ä½“
    use_chinese = setup_chinese_font()
    
    # åŠ è½½æ•°æ®
    df = load_data()
    if df is None:
        return
    
    # å‡†å¤‡ç‰¹å¾
    X, y, feature_cols = prepare_features(df)
    
    # æ‰§è¡Œåˆ†ç±»åˆ†æ
    results = perform_classification(X, y)
    
    # åˆ›å»ºä¿®å¤åçš„å¯è§†åŒ–
    create_fixed_visualization(df, X, y, results, use_chinese)
    
    print("\n" + "="*60)
    print("ğŸ‰ ä¸­æ–‡å­—ä½“ä¿®å¤å®Œæˆ!")
    print("="*60)
    print("\nç”Ÿæˆçš„æ–‡ä»¶:")
    print("  ğŸ“Š ../results/complete_mat_analysis_fixed.png - ä¿®å¤åçš„åˆ†æå¯è§†åŒ–")
    
    if use_chinese:
        print("\nâœ“ ä¸­æ–‡æ˜¾ç¤ºæ­£å¸¸")
    else:
        print("\nâš ï¸ ä½¿ç”¨è‹±æ–‡æ ‡ç­¾ï¼ˆç³»ç»Ÿä¸æ”¯æŒä¸­æ–‡å­—ä½“ï¼‰")

if __name__ == "__main__":
    main()
